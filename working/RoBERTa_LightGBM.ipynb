{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RoBERTa_LightGBM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goya5858/commonlitreadabilityprize/blob/main/working/RoBERTa_LightGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO8Aj3H68nPa"
      },
      "source": [
        "# Install libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpXmMzbt_oWU",
        "outputId": "241a3db8-4614-4624-f992-b785f882a0f0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC5H84YW7Oj3",
        "outputId": "7dc075bf-6ff7-4b22-df93-ae8c20f9c213"
      },
      "source": [
        "compe_name = 'commonlitreadabilityprize'\n",
        "%cd /content/drive/MyDrive/kaggle/works/$compe_name/working/\n",
        "\n",
        "!pip install -q -q -q -U albumentations\n",
        "!pip install -q -q -q -U torch\n",
        "!pip install -q -q -q timm\n",
        "!pip install -q -q -q pytorch_lightning\n",
        "!pip install -q -q -q -U transformers\n",
        "!pip install -q -q -q -U sentencepiece\n",
        "\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "import warnings\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2\n",
        "import PIL.Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import timm\n",
        "\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop=stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob,Word\n",
        "from collections import Counter\n",
        "import string\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from torch.nn import MSELoss\n",
        "\n",
        "from torch.cuda import amp\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection\n",
        "\n",
        "import transformers\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "\n",
        "from transformers import *\n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    #tf.random.set_seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/kaggle/works/commonlitreadabilityprize/working\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W3SxA_QKVvX"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E36RCH0tqF2H"
      },
      "source": [
        "DEVICE = 'cuda:0'\n",
        "\n",
        "get_tokenizer = RobertaTokenizer\n",
        "#get_model     = BartForSequenceClassification\n",
        "#get_tokenizer = AutoTokenizer #RobertaTokenizer より汎用性ある気がする　使い勝手的な面で\n",
        "#get_model     = AutoModel\n",
        "#get_model     = RobertaForSequenceClassification\n",
        "get_model     = RobertaModel\n",
        "\n",
        "VOCAB_PATH = 'roberta-base'\n",
        "MODEL_PATH = 'roberta-base'\n",
        "#VOCAB_PATH = 'bert-base-uncased'\n",
        "#MODEL_PATH = 'bert-base-uncased'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-n9rpp-77hDn",
        "outputId": "c60bc31e-f950-42d9-b2df-22bd29a346f4"
      },
      "source": [
        "ROOT = f\"../input/{compe_name}/\"\n",
        "MODEL_ROOT = \"../input/models/RoBERTa_small\"\n",
        "\n",
        "df = pd.read_csv(ROOT+'train.csv')\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c12129c31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When the young people returned to the ballroom...</td>\n",
              "      <td>-0.340259</td>\n",
              "      <td>0.464009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85aa80a4c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
              "      <td>-0.315372</td>\n",
              "      <td>0.480805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b69ac6792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As Roger had predicted, the snow departed as q...</td>\n",
              "      <td>-0.580118</td>\n",
              "      <td>0.476676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dd1000b26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>And outside before the palace a great garden w...</td>\n",
              "      <td>-1.054013</td>\n",
              "      <td>0.450007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37c1b32fb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Once upon a time there were Three Bears who li...</td>\n",
              "      <td>0.247197</td>\n",
              "      <td>0.510845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id url_legal  ...    target standard_error\n",
              "0  c12129c31       NaN  ... -0.340259       0.464009\n",
              "1  85aa80a4c       NaN  ... -0.315372       0.480805\n",
              "2  b69ac6792       NaN  ... -0.580118       0.476676\n",
              "3  dd1000b26       NaN  ... -1.054013       0.450007\n",
              "4  37c1b32fb       NaN  ...  0.247197       0.510845\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "3lijAHPgNmyi",
        "outputId": "2ece22df-8a65-449a-8d84-8f0d320acf71"
      },
      "source": [
        "test_df = pd.read_csv(ROOT+'test.csv')\n",
        "test_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c0f722661</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My hope lay in Jack's promise that he would ke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>f0953f0a5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Dotty continued to go to Mrs. Gray's every nig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0df072751</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It was a bright and cheerful scene that greete...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>04caf4e0c</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Cell_division</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>Cell division is the process by which a parent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0e63f8bea</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Debugging</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>Debugging is the process of finding and resolv...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                            excerpt\n",
              "0  c0f722661  ...  My hope lay in Jack's promise that he would ke...\n",
              "1  f0953f0a5  ...  Dotty continued to go to Mrs. Gray's every nig...\n",
              "2  0df072751  ...  It was a bright and cheerful scene that greete...\n",
              "3  04caf4e0c  ...  Cell division is the process by which a parent...\n",
              "4  0e63f8bea  ...  Debugging is the process of finding and resolv...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfVQi9GWNxfE",
        "outputId": "73f8f2ff-3570-43a2-9468-e001f9aeba8f"
      },
      "source": [
        "def prep_text(text_df):\n",
        "    text_df = text_df.str.replace(\"\\n\",\"\",regex=False)\n",
        "    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\n",
        "\n",
        "df['excerpt']      = prep_text(df['excerpt'])\n",
        "test_df['excerpt'] = prep_text(test_df['excerpt'])\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = df['excerpt'].apply(lambda x: len(x.split())).max()\n",
        "\n",
        "tokenizer = get_tokenizer.from_pretrained(VOCAB_PATH,\n",
        "                                          model_max_length=MAX_SEQUENCE_LENGTH\n",
        "                                          )\n",
        "df['token']          = df['excerpt'].apply(tokenizer)\n",
        "test_df['token']   = test_df['excerpt'].apply(tokenizer)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (222 > 205). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "oQBouRBXaJmC",
        "outputId": "f446bb74-7e0c-40c5-c28b-87c032bf3877"
      },
      "source": [
        "RoBERTa_pred = pd.read_csv(ROOT+'RoBERTa_oofs.csv')\n",
        "RoBERTa_pred"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>seed_0</th>\n",
              "      <th>seed_7</th>\n",
              "      <th>seed_42</th>\n",
              "      <th>seed_88</th>\n",
              "      <th>seed_100</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.259451</td>\n",
              "      <td>-0.062605</td>\n",
              "      <td>0.079190</td>\n",
              "      <td>-0.320383</td>\n",
              "      <td>-0.210586</td>\n",
              "      <td>-0.340259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.434118</td>\n",
              "      <td>-0.200020</td>\n",
              "      <td>-0.262286</td>\n",
              "      <td>-0.051768</td>\n",
              "      <td>-0.131072</td>\n",
              "      <td>-0.315372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.364411</td>\n",
              "      <td>-0.639652</td>\n",
              "      <td>-0.425685</td>\n",
              "      <td>-0.624188</td>\n",
              "      <td>-0.569017</td>\n",
              "      <td>-0.580118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.390543</td>\n",
              "      <td>-1.851582</td>\n",
              "      <td>-1.599446</td>\n",
              "      <td>-1.322094</td>\n",
              "      <td>-1.352633</td>\n",
              "      <td>-1.054013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.206669</td>\n",
              "      <td>0.280685</td>\n",
              "      <td>0.082368</td>\n",
              "      <td>0.317499</td>\n",
              "      <td>0.263067</td>\n",
              "      <td>0.247197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2829</th>\n",
              "      <td>0.573117</td>\n",
              "      <td>0.539531</td>\n",
              "      <td>0.275756</td>\n",
              "      <td>0.464814</td>\n",
              "      <td>0.496079</td>\n",
              "      <td>1.711390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2830</th>\n",
              "      <td>-0.201056</td>\n",
              "      <td>0.321773</td>\n",
              "      <td>0.230156</td>\n",
              "      <td>-0.000241</td>\n",
              "      <td>0.137118</td>\n",
              "      <td>0.189476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2831</th>\n",
              "      <td>-0.321227</td>\n",
              "      <td>-0.451728</td>\n",
              "      <td>-0.603753</td>\n",
              "      <td>-0.653499</td>\n",
              "      <td>-0.490357</td>\n",
              "      <td>0.255209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2832</th>\n",
              "      <td>-1.136917</td>\n",
              "      <td>-1.241406</td>\n",
              "      <td>-1.173482</td>\n",
              "      <td>-1.164049</td>\n",
              "      <td>-0.858960</td>\n",
              "      <td>-0.215279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2833</th>\n",
              "      <td>0.108262</td>\n",
              "      <td>-0.302992</td>\n",
              "      <td>-0.253670</td>\n",
              "      <td>-0.091877</td>\n",
              "      <td>0.331138</td>\n",
              "      <td>0.300779</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2834 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        seed_0    seed_7   seed_42   seed_88  seed_100    target\n",
              "0    -0.259451 -0.062605  0.079190 -0.320383 -0.210586 -0.340259\n",
              "1    -0.434118 -0.200020 -0.262286 -0.051768 -0.131072 -0.315372\n",
              "2    -0.364411 -0.639652 -0.425685 -0.624188 -0.569017 -0.580118\n",
              "3    -1.390543 -1.851582 -1.599446 -1.322094 -1.352633 -1.054013\n",
              "4     0.206669  0.280685  0.082368  0.317499  0.263067  0.247197\n",
              "...        ...       ...       ...       ...       ...       ...\n",
              "2829  0.573117  0.539531  0.275756  0.464814  0.496079  1.711390\n",
              "2830 -0.201056  0.321773  0.230156 -0.000241  0.137118  0.189476\n",
              "2831 -0.321227 -0.451728 -0.603753 -0.653499 -0.490357  0.255209\n",
              "2832 -1.136917 -1.241406 -1.173482 -1.164049 -0.858960 -0.215279\n",
              "2833  0.108262 -0.302992 -0.253670 -0.091877  0.331138  0.300779\n",
              "\n",
              "[2834 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrdB7YFV7nmT"
      },
      "source": [
        "class CLPDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        super().__init__()\n",
        "        self.token  = df.token\n",
        "        self.labels = df.target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "  \n",
        "    def __getitem__(self, idx):\n",
        "        if len(self.token.iloc[0]) == 2:\n",
        "            return (\n",
        "                    torch.tensor(self.token.iloc[idx].input_ids), \\\n",
        "                    #torch.tensor(self.token.iloc[idx].token_type_ids), \\\n",
        "                    torch.tensor(self.token.iloc[idx].attention_mask)\n",
        "                  ), \\\n",
        "                  torch.tensor(self.labels.iloc[idx])\n",
        "        if len(self.token.iloc[idx]) == 3:\n",
        "            return (\n",
        "                    torch.tensor(self.token.iloc[idx].input_ids), \\\n",
        "                    torch.tensor(self.token.iloc[idx].token_type_ids), \\\n",
        "                    torch.tensor(self.token.iloc[idx].attention_mask)\n",
        "                    ), \\\n",
        "                  torch.tensor(self.labels.iloc[idx])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96CkS0Vc8xTq"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    inputs, labels = zip(*batch)\n",
        "    try:\n",
        "        ids, types, masks = zip(*inputs)\n",
        "        ids   = pad_sequence(ids, batch_first=True).to(DEVICE)\n",
        "        types = pad_sequence(types, batch_first=True).to(DEVICE)\n",
        "        masks = pad_sequence(masks, batch_first=True).to(DEVICE)\n",
        "        labels= torch.tensor(labels, dtype=torch.float).to(DEVICE)\n",
        "        return {\n",
        "                    \"input_ids\"      : ids, \\\n",
        "                    \"token_type_ids\" : types, \\\n",
        "                    \"attention_mask\" : masks\n",
        "                  }, \\\n",
        "                  labels\n",
        "    except ValueError:\n",
        "        ids, masks = zip(*inputs)\n",
        "        ids   = pad_sequence(ids, batch_first=True).to(DEVICE)\n",
        "        #types = pad_sequence(types, batch_first=True).to(DEVICE)\n",
        "        masks = pad_sequence(masks, batch_first=True).to(DEVICE)\n",
        "        labels= torch.tensor(labels, dtype=torch.float).to(DEVICE)\n",
        "        return {\n",
        "                    \"input_ids\"      : ids, \\\n",
        "                    #\"token_type_ids\" : types, \\\n",
        "                    \"attention_mask\" : masks\n",
        "                  }, \\\n",
        "                  labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1bVE8fUQImk"
      },
      "source": [
        "# trainデータを、targetの値をビニングした値を元に層化fold\n",
        "def create_folds(data, num_splits):\n",
        "    # we create a new column called kfold and fill it with -1\n",
        "    folds = pd.DataFrame( np.ones( (data.shape[0],1) )*-1, columns=['kfold'] )\n",
        "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
        "\n",
        "    bins = pd.cut(\n",
        "          data[\"target\"], bins=num_bins, labels=False\n",
        "          )\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=bins)):\n",
        "        folds.iloc[v_] = int(f)\n",
        "\n",
        "    return folds"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAwOwHYgWswt"
      },
      "source": [
        "def get_dataloaders(df, folds, n_fold, BATCH_SIZE):\n",
        "    #folds = create_folds(df, num_splits=NUM_FOLDS)\n",
        "    train_df = df[(folds['kfold']!=n_fold)]\n",
        "    valid_df = df[(folds['kfold']==n_fold)]\n",
        "\n",
        "    train_dataset = CLPDataset(df=train_df)\n",
        "    valid_dataset = CLPDataset(df=valid_df)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset = train_dataset,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        shuffle = True,\n",
        "        collate_fn=collate_fn,\n",
        "        #num_workers = -1\n",
        "        )\n",
        "    valid_loader = DataLoader(\n",
        "        dataset = valid_dataset,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        shuffle = False,\n",
        "        collate_fn=collate_fn,\n",
        "        #num_workers = -1\n",
        "        )\n",
        "    return train_loader, valid_loader"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyZiQsB_bRge"
      },
      "source": [
        "# LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfsTZ1R9bUoa"
      },
      "source": [
        "import lightgbm as lgb\n",
        "data_cols = ['seed_0', 'seed_7', 'seed_42', 'seed_88', 'seed_100']\n",
        "target_col = 'target'\n",
        "\n",
        "def get_dataset(df, folds, n_fold):\n",
        "    trn_df = df[(folds['kfold']!=n_fold)]\n",
        "    val_df = df[(folds['kfold']==n_fold)]\n",
        "    trn_dataset = lgb.Dataset(\n",
        "                                data = trn_df[data_cols],\n",
        "                                label = trn_df[target_col]\n",
        "                             )\n",
        "    val_dataset = lgb.Dataset(\n",
        "                                data = val_df[data_cols],\n",
        "                                label = val_df[target_col],\n",
        "                                reference = trn_dataset\n",
        "                             )\n",
        "    return trn_dataset, val_dataset, val_df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov_6lY9seErb"
      },
      "source": [
        "params = {\n",
        " 'reg_alpha': 6.147694913504962,\n",
        " 'reg_lambda': 0.002457826062076097,\n",
        " 'colsample_bytree': 0.3,\n",
        " 'subsample': 0.8,\n",
        " 'learning_rate': 1e-2,\n",
        " 'max_depth': 20,\n",
        " 'num_leaves': 111,\n",
        " 'min_child_samples': 285,\n",
        " 'random_state': 42,\n",
        " 'verbose':-1,\n",
        " 'n_estimators': 10000,\n",
        " 'metric': 'rmse',\n",
        " 'cat_smooth': 39}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxPlN34ui0DD"
      },
      "source": [
        "def loss_fn(pred, labels):\n",
        "  return MSELoss()(pred.view(-1), labels.view(-1))\n",
        "\n",
        "def train_fold(folds, n_fold, seed):\n",
        "    lgb_train,lgb_valid, val_df = get_dataset(RoBERTa_pred, folds, n_fold)\n",
        "    lgb_model = lgb.train(params,\n",
        "                          lgb_train, \n",
        "                          valid_sets=[lgb_train,lgb_valid],\n",
        "                          verbose_eval=500,\n",
        "                          early_stopping_rounds=800,\n",
        "                          )\n",
        "    pred_fold = lgb_model.predict(val_df[data_cols])\n",
        "    model_path = f\"../input/models/RoBERTa_model/seed-{seed}-fold-{n_fold}-lgbmodel.txt\"\n",
        "    lgb_model.save_model( model_path)\n",
        "    del lgb_model\n",
        "    gc.collect()\n",
        "    return pred_fold"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htzvlSdekVHh"
      },
      "source": [
        "def train_seed(seed):\n",
        "    seed_everything(seed)\n",
        "    folds = create_folds(df, num_splits=NUM_FOLDS)\n",
        "    oof   = np.zeros(df['target'].shape)\n",
        "\n",
        "    for n_fold in range(NUM_FOLDS):\n",
        "        best_pred = train_fold(folds, n_fold, seed)\n",
        "        oof[(folds['kfold']==n_fold)] = best_pred\n",
        "        print('-='*20)\n",
        "\n",
        "    print('%'*50)\n",
        "    oof_score = loss_fn(pred   = torch.tensor(oof).to(DEVICE),\n",
        "                                  labels = torch.tensor(df['target'].values).to(DEVICE) )\n",
        "    print(f'oof_score_{seed} :', np.sqrt( oof_score.cpu().detach().numpy()))\n",
        "    return np.sqrt( oof_score.cpu().detach().numpy() )"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfoNRh0ZkhIo",
        "outputId": "4f64f563-6433-4099-b36a-9c491fe4aea1"
      },
      "source": [
        "SEEDs = [0, 7, 42, 88, 100]\n",
        "NUM_FOLDS = 5\n",
        "\n",
        "MAX_WORDS = df[\"excerpt\"].apply(lambda x: len(x.split())).max()\n",
        "\n",
        "oof_scores  = []\n",
        "for seed in SEEDs:\n",
        "  print(f'--------------- SEED {seed} is set ---------------')\n",
        "  oof_score_for_seed = train_seed(seed)\n",
        "  oof_scores.append(oof_score_for_seed)\n",
        "\n",
        "print('&%&%'*30)\n",
        "print(\"all_oof_score_avg : \", np.mean(oof_scores) )\n",
        "print(\"all_oof_scores : \", oof_scores)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------- SEED 0 is set ---------------\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.482775\tvalid_1's rmse: 0.484869\n",
            "[1000]\ttraining's rmse: 0.480806\tvalid_1's rmse: 0.485269\n",
            "Early stopping, best iteration is:\n",
            "[453]\ttraining's rmse: 0.483238\tvalid_1's rmse: 0.484839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.480599\tvalid_1's rmse: 0.492094\n",
            "[1000]\ttraining's rmse: 0.478646\tvalid_1's rmse: 0.490834\n",
            "[1500]\ttraining's rmse: 0.477821\tvalid_1's rmse: 0.490753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.477343\tvalid_1's rmse: 0.490759\n",
            "Early stopping, best iteration is:\n",
            "[1673]\ttraining's rmse: 0.477656\tvalid_1's rmse: 0.490737\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.48234\tvalid_1's rmse: 0.483067\n",
            "[1000]\ttraining's rmse: 0.480594\tvalid_1's rmse: 0.481595\n",
            "[1500]\ttraining's rmse: 0.479735\tvalid_1's rmse: 0.480913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.479177\tvalid_1's rmse: 0.48067\n",
            "[2500]\ttraining's rmse: 0.478753\tvalid_1's rmse: 0.480518\n",
            "[3000]\ttraining's rmse: 0.478436\tvalid_1's rmse: 0.480564\n",
            "Early stopping, best iteration is:\n",
            "[2513]\ttraining's rmse: 0.478749\tvalid_1's rmse: 0.480514\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.479339\tvalid_1's rmse: 0.502238\n",
            "[1000]\ttraining's rmse: 0.477437\tvalid_1's rmse: 0.502653\n",
            "Early stopping, best iteration is:\n",
            "[431]\ttraining's rmse: 0.480027\tvalid_1's rmse: 0.502147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.479302\tvalid_1's rmse: 0.493143\n",
            "[1000]\ttraining's rmse: 0.477767\tvalid_1's rmse: 0.492645\n",
            "[1500]\ttraining's rmse: 0.477115\tvalid_1's rmse: 0.492617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.476734\tvalid_1's rmse: 0.492664\n",
            "Early stopping, best iteration is:\n",
            "[1447]\ttraining's rmse: 0.477167\tvalid_1's rmse: 0.492599\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "oof_score_0 : 0.49023666845788527\n",
            "--------------- SEED 7 is set ---------------\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.477522\tvalid_1's rmse: 0.504474\n",
            "[1000]\ttraining's rmse: 0.475617\tvalid_1's rmse: 0.504607\n",
            "Early stopping, best iteration is:\n",
            "[570]\ttraining's rmse: 0.477036\tvalid_1's rmse: 0.504349\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.482845\tvalid_1's rmse: 0.487446\n",
            "[1000]\ttraining's rmse: 0.481274\tvalid_1's rmse: 0.486317\n",
            "[1500]\ttraining's rmse: 0.480533\tvalid_1's rmse: 0.486096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.48017\tvalid_1's rmse: 0.486007\n",
            "[2500]\ttraining's rmse: 0.479884\tvalid_1's rmse: 0.485909\n",
            "[3000]\ttraining's rmse: 0.47964\tvalid_1's rmse: 0.485765\n",
            "[3500]\ttraining's rmse: 0.479564\tvalid_1's rmse: 0.485708\n",
            "[4000]\ttraining's rmse: 0.479515\tvalid_1's rmse: 0.4857\n",
            "[4500]\ttraining's rmse: 0.479451\tvalid_1's rmse: 0.485649\n",
            "[5000]\ttraining's rmse: 0.479393\tvalid_1's rmse: 0.485612\n",
            "[5500]\ttraining's rmse: 0.479317\tvalid_1's rmse: 0.485563\n",
            "[6000]\ttraining's rmse: 0.479251\tvalid_1's rmse: 0.485529\n",
            "[6500]\ttraining's rmse: 0.479191\tvalid_1's rmse: 0.485491\n",
            "[7000]\ttraining's rmse: 0.479139\tvalid_1's rmse: 0.48546\n",
            "[7500]\ttraining's rmse: 0.479124\tvalid_1's rmse: 0.485453\n",
            "Early stopping, best iteration is:\n",
            "[7095]\ttraining's rmse: 0.479127\tvalid_1's rmse: 0.48545\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.483855\tvalid_1's rmse: 0.481665\n",
            "[1000]\ttraining's rmse: 0.482108\tvalid_1's rmse: 0.481475\n",
            "[1500]\ttraining's rmse: 0.481274\tvalid_1's rmse: 0.481152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.480753\tvalid_1's rmse: 0.481005\n",
            "[2500]\ttraining's rmse: 0.480407\tvalid_1's rmse: 0.480855\n",
            "[3000]\ttraining's rmse: 0.48012\tvalid_1's rmse: 0.480736\n",
            "[3500]\ttraining's rmse: 0.479841\tvalid_1's rmse: 0.480723\n",
            "[4000]\ttraining's rmse: 0.479619\tvalid_1's rmse: 0.480726\n",
            "[4500]\ttraining's rmse: 0.479439\tvalid_1's rmse: 0.480737\n",
            "Early stopping, best iteration is:\n",
            "[4145]\ttraining's rmse: 0.479558\tvalid_1's rmse: 0.480681\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.478717\tvalid_1's rmse: 0.498081\n",
            "[1000]\ttraining's rmse: 0.476985\tvalid_1's rmse: 0.497621\n",
            "[1500]\ttraining's rmse: 0.476053\tvalid_1's rmse: 0.497691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[1097]\ttraining's rmse: 0.47677\tvalid_1's rmse: 0.497576\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.482432\tvalid_1's rmse: 0.488023\n",
            "[1000]\ttraining's rmse: 0.48072\tvalid_1's rmse: 0.487777\n",
            "[1500]\ttraining's rmse: 0.479895\tvalid_1's rmse: 0.488029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[776]\ttraining's rmse: 0.481279\tvalid_1's rmse: 0.487669\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "oof_score_7 : 0.4912357219241375\n",
            "--------------- SEED 42 is set ---------------\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.480184\tvalid_1's rmse: 0.493693\n",
            "[1000]\ttraining's rmse: 0.478737\tvalid_1's rmse: 0.493669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1500]\ttraining's rmse: 0.478181\tvalid_1's rmse: 0.493547\n",
            "[2000]\ttraining's rmse: 0.477882\tvalid_1's rmse: 0.493427\n",
            "[2500]\ttraining's rmse: 0.477658\tvalid_1's rmse: 0.49346\n",
            "Early stopping, best iteration is:\n",
            "[1959]\ttraining's rmse: 0.477898\tvalid_1's rmse: 0.493422\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.475448\tvalid_1's rmse: 0.513427\n",
            "[1000]\ttraining's rmse: 0.473398\tvalid_1's rmse: 0.514344\n",
            "Early stopping, best iteration is:\n",
            "[417]\ttraining's rmse: 0.476433\tvalid_1's rmse: 0.513016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.481275\tvalid_1's rmse: 0.491661\n",
            "[1000]\ttraining's rmse: 0.479414\tvalid_1's rmse: 0.491726\n",
            "Early stopping, best iteration is:\n",
            "[549]\ttraining's rmse: 0.480939\tvalid_1's rmse: 0.491582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.48773\tvalid_1's rmse: 0.465549\n",
            "[1000]\ttraining's rmse: 0.485966\tvalid_1's rmse: 0.463941\n",
            "[1500]\ttraining's rmse: 0.485038\tvalid_1's rmse: 0.463695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.484472\tvalid_1's rmse: 0.463463\n",
            "[2500]\ttraining's rmse: 0.484127\tvalid_1's rmse: 0.463288\n",
            "[3000]\ttraining's rmse: 0.483911\tvalid_1's rmse: 0.463159\n",
            "[3500]\ttraining's rmse: 0.483748\tvalid_1's rmse: 0.463069\n",
            "[4000]\ttraining's rmse: 0.483618\tvalid_1's rmse: 0.463008\n",
            "[4500]\ttraining's rmse: 0.483618\tvalid_1's rmse: 0.463008\n",
            "Early stopping, best iteration is:\n",
            "[3928]\ttraining's rmse: 0.483618\tvalid_1's rmse: 0.463008\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.479707\tvalid_1's rmse: 0.492847\n",
            "[1000]\ttraining's rmse: 0.47795\tvalid_1's rmse: 0.49291\n",
            "Early stopping, best iteration is:\n",
            "[675]\ttraining's rmse: 0.478822\tvalid_1's rmse: 0.492685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "oof_score_42 : 0.4910134752695298\n",
            "--------------- SEED 88 is set ---------------\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.479672\tvalid_1's rmse: 0.499004\n",
            "[1000]\ttraining's rmse: 0.477933\tvalid_1's rmse: 0.499622\n",
            "Early stopping, best iteration is:\n",
            "[496]\ttraining's rmse: 0.479708\tvalid_1's rmse: 0.498997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.482746\tvalid_1's rmse: 0.482658\n",
            "[1000]\ttraining's rmse: 0.480763\tvalid_1's rmse: 0.482235\n",
            "[1500]\ttraining's rmse: 0.479998\tvalid_1's rmse: 0.482058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.479576\tvalid_1's rmse: 0.481789\n",
            "[2500]\ttraining's rmse: 0.47931\tvalid_1's rmse: 0.481596\n",
            "[3000]\ttraining's rmse: 0.479118\tvalid_1's rmse: 0.481461\n",
            "[3500]\ttraining's rmse: 0.479014\tvalid_1's rmse: 0.481372\n",
            "[4000]\ttraining's rmse: 0.479002\tvalid_1's rmse: 0.481362\n",
            "Early stopping, best iteration is:\n",
            "[3616]\ttraining's rmse: 0.479002\tvalid_1's rmse: 0.481362\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.482005\tvalid_1's rmse: 0.484873\n",
            "[1000]\ttraining's rmse: 0.480197\tvalid_1's rmse: 0.484271\n",
            "[1500]\ttraining's rmse: 0.479504\tvalid_1's rmse: 0.48428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[1047]\ttraining's rmse: 0.480099\tvalid_1's rmse: 0.484249\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.477185\tvalid_1's rmse: 0.50381\n",
            "[1000]\ttraining's rmse: 0.475756\tvalid_1's rmse: 0.503216\n",
            "[1500]\ttraining's rmse: 0.475109\tvalid_1's rmse: 0.502815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.474562\tvalid_1's rmse: 0.502582\n",
            "[2500]\ttraining's rmse: 0.474131\tvalid_1's rmse: 0.502477\n",
            "[3000]\ttraining's rmse: 0.473774\tvalid_1's rmse: 0.502467\n",
            "[3500]\ttraining's rmse: 0.473471\tvalid_1's rmse: 0.502464\n",
            "[4000]\ttraining's rmse: 0.473248\tvalid_1's rmse: 0.50249\n",
            "Early stopping, best iteration is:\n",
            "[3259]\ttraining's rmse: 0.473597\tvalid_1's rmse: 0.502456\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.482337\tvalid_1's rmse: 0.485792\n",
            "[1000]\ttraining's rmse: 0.480507\tvalid_1's rmse: 0.485282\n",
            "[1500]\ttraining's rmse: 0.479554\tvalid_1's rmse: 0.485404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[915]\ttraining's rmse: 0.480709\tvalid_1's rmse: 0.485261\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "oof_score_88 : 0.49054474600112347\n",
            "--------------- SEED 100 is set ---------------\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.479201\tvalid_1's rmse: 0.500257\n",
            "[1000]\ttraining's rmse: 0.477376\tvalid_1's rmse: 0.499504\n",
            "[1500]\ttraining's rmse: 0.476442\tvalid_1's rmse: 0.499392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.475743\tvalid_1's rmse: 0.499579\n",
            "Early stopping, best iteration is:\n",
            "[1263]\ttraining's rmse: 0.476843\tvalid_1's rmse: 0.499367\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.481009\tvalid_1's rmse: 0.493104\n",
            "[1000]\ttraining's rmse: 0.479109\tvalid_1's rmse: 0.493758\n",
            "Early stopping, best iteration is:\n",
            "[495]\ttraining's rmse: 0.481058\tvalid_1's rmse: 0.493092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.484931\tvalid_1's rmse: 0.478583\n",
            "[1000]\ttraining's rmse: 0.482955\tvalid_1's rmse: 0.479097\n",
            "Early stopping, best iteration is:\n",
            "[468]\ttraining's rmse: 0.485239\tvalid_1's rmse: 0.47852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.476575\tvalid_1's rmse: 0.508826\n",
            "[1000]\ttraining's rmse: 0.475174\tvalid_1's rmse: 0.507377\n",
            "[1500]\ttraining's rmse: 0.474502\tvalid_1's rmse: 0.507113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000]\ttraining's rmse: 0.47407\tvalid_1's rmse: 0.507026\n",
            "[2500]\ttraining's rmse: 0.473805\tvalid_1's rmse: 0.507086\n",
            "Early stopping, best iteration is:\n",
            "[1919]\ttraining's rmse: 0.474124\tvalid_1's rmse: 0.507022\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "Training until validation scores don't improve for 800 rounds.\n",
            "[500]\ttraining's rmse: 0.481613\tvalid_1's rmse: 0.488479\n",
            "[1000]\ttraining's rmse: 0.479783\tvalid_1's rmse: 0.488334\n",
            "[1500]\ttraining's rmse: 0.478892\tvalid_1's rmse: 0.488462\n",
            "Early stopping, best iteration is:\n",
            "[706]\ttraining's rmse: 0.480579\tvalid_1's rmse: 0.488301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "oof_score_100 : 0.49336368076124515\n",
            "&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%&%\n",
            "all_oof_score_avg :  0.4912788584827842\n",
            "all_oof_scores :  [0.49023666845788527, 0.4912357219241375, 0.4910134752695298, 0.49054474600112347, 0.49336368076124515]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}